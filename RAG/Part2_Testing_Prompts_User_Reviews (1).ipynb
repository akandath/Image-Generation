{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade llama-index\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPkpB0pChCTu",
        "outputId": "ebde230f-70ef-4d1f-8a27-d787cc109392"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.12.2)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.54.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.27.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic<2.10.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.9.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.2->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.1.0)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.16-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.2->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.10.0,>=2.7.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (3.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.2->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.2->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.12.2-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.3.2-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.3.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.16-py3-none-any.whl (14 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Installing collected packages: striprtf, llama-cloud, llama-index-legacy, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed llama-cloud-0.1.6 llama-index-0.12.2 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-openai-0.3.2 llama-index-multi-modal-llms-openai-0.3.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.0 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.16 striprtf-0.0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyRpo1m-fwvO",
        "outputId": "f36c3859-4049-4dda-98a0-07e5921a06e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (5.5.6)\n",
            "Requirement already satisfied: langchain_experimental in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: llama-index-vector-stores-pinecone in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.14)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel) (6.3.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain_experimental) (0.3.21)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-pinecone) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.9)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.143)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.0.0->ipykernel) (4.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.8->langchain_community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_experimental) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.2.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (3.9.1)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (0.8.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (1.16.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (5.7.2)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (24.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel) (2.8.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_experimental) (3.0.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel) (4.3.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-pinecone) (8.1.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.8->langchain_community) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client->ipykernel) (1.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "\n",
        "%pip install ipykernel langchain_experimental llama-index-vector-stores-pinecone ipykernel PyMuPDF pinecone-client pypdf faiss-cpu langchain_community transformers sentence_transformers\n",
        "\n",
        "import fitz\n",
        "\n",
        "import os, io, json, transformers, pinecone, pypdf, faiss, sqlite3, langchain_community, langchain, openai, math, time, nltk, torch, huggingface_hub\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from dotenv import load_dotenv\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain import document_loaders, embeddings\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "\n",
        "#from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from llama_index.core.schema import TextNode\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec, Pinecone         # vector store\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    SimpleDirectoryReader\n",
        ")\n",
        "\n",
        "from llama_index.core.extractors import (\n",
        "    QuestionsAnsweredExtractor,\n",
        "    TitleExtractor,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "YFYvaV3AfwvS",
        "outputId": "97794b57-109d-4e7f-9845-6080d6b29d74"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PineconeConfigurationError",
          "evalue": "You haven't specified an Api-Key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPineconeConfigurationError\u001b[0m                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-dfc7b726beaa>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menvironment\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PINECONE_ENV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HF_TOKEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPinecone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpinecone_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/control/pinecone.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, config, additional_headers, pool_threads, index_api, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             self.config = PineconeConfig.build(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/config/pinecone_config.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(api_key, host, additional_headers, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ignoring PINECONE_ADDITIONAL_HEADERS: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         return ConfigBuilder.build(\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pinecone/config/config.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(api_key, host, proxy_url, proxy_headers, ssl_ca_certs, ssl_verify, additional_headers, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You haven't specified an Api-Key.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You haven't specified a host.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPineconeConfigurationError\u001b[0m: You haven't specified an Api-Key."
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "openai = os.getenv('OPENAI_API_KEY')\n",
        "pinecone_api_key =os.getenv('PINECONE_API_KEY')\n",
        "environment =os.getenv('PINECONE_ENV')\n",
        "HF_TOKEN = os.getenv('HF_TOKEN')\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sOIGQbI6fwvS"
      },
      "outputs": [],
      "source": [
        "import json, os, io, re, requests, fitz\n",
        "import requests\n",
        "from langchain_text_splitters import RecursiveJsonSplitter\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "#from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    SimpleDirectoryReader\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
        "from llama_index.core.schema import TextNode\n",
        "from llama_index.core.extractors import (\n",
        "    QuestionsAnsweredExtractor,\n",
        "    KeywordExtractor,\n",
        "    SummaryExtractor,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFAPQbsSgEna"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XDnfrQnqfwvT"
      },
      "outputs": [],
      "source": [
        "#doc = fitz.open(r\"C:\\GenAIhw3\\product_info_pdf.pdf\")\n",
        "doc = fitz.open(r\"/content/product_info_pdf.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v9Rz_Z_VfwvT"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "text_parser = SentenceSplitter(\n",
        "    chunk_size=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_rDaeH2fwvT",
        "outputId": "8db36779-0d8c-4dac-dfa9-32042c6b6a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zX4WiEFZfwvU"
      },
      "outputs": [],
      "source": [
        "text_chunks = []\n",
        "doc_idxs = []\n",
        "\n",
        "\n",
        "for doc_idx, page in enumerate(doc):\n",
        "    page_text = page.get_text(\"text\")\n",
        "    cur_text_chunks = text_parser.split_text(page_text)\n",
        "    text_chunks.extend(cur_text_chunks)\n",
        "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "63EEV278fwvU"
      },
      "outputs": [],
      "source": [
        "nodes = []\n",
        "\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "    node = TextNode(\n",
        "        text=text_chunk,\n",
        "    )\n",
        "    src_doc_idx = doc_idxs[idx]\n",
        "    src_page = doc[src_doc_idx]\n",
        "    nodes.append(node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2uK4Zm6fwvU"
      },
      "source": [
        "### **Create the vector store using chosen similarity metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3lFE49izfwvV"
      },
      "outputs": [],
      "source": [
        "use_serverless = os.environ.get(\"USE_SERVERLESS\", \"False\").lower() == \"true\"\n",
        "\n",
        "if use_serverless:\n",
        "    spec = pinecone.ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "else:\n",
        "    spec = pinecone.PodSpec(environment=environment)\n",
        "\n",
        "index_name = \"hw05\"\n",
        "\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wniTcN92fwvV"
      },
      "outputs": [],
      "source": [
        "dimensions = 1536\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=dimensions,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")\n",
        "\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYJVz6NVfwvV"
      },
      "source": [
        "### **choose a similarity metric to use for the vector store:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEa1YoVkfwvV",
        "outputId": "0bfe6194-e735-4aa4-d1cb-83afed83befe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw05\n",
            "cmu-handbook-alternative\n",
            "cmu-handbook-recursive\n",
            "cmu-handbook-semantic\n",
            "hw02\n"
          ]
        }
      ],
      "source": [
        "for index in pc.list_indexes():\n",
        "    print(index['name'])\n",
        "\n",
        "pc.describe_index(\"hw05\")\n",
        "\n",
        "pc_index = pc.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(pinecone_index=pc_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM80dU9TfwvW",
        "outputId": "d2bee546-a1f2-435c-8830-19347c309702"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "\n",
        "pc_index.describe_index_stats()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9wtkVWufwvW",
        "outputId": "19bbe112-01b6-4517-9bf2-db01378984a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 1/37 [00:00<00:18,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▎        | 5/37 [00:00<00:05,  6.19it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 7/37 [00:01<00:03,  8.30it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 9/37 [00:01<00:03,  7.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 11/37 [00:02<00:07,  3.66it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 15/37 [00:02<00:04,  5.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▊     | 18/37 [00:02<00:02,  7.26it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 20/37 [00:03<00:02,  7.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 22/37 [00:03<00:01,  8.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 24/37 [00:03<00:01,  8.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 26/37 [00:03<00:01,  9.49it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 28/37 [00:04<00:01,  8.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 30/37 [00:04<00:00, 10.00it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 32/37 [00:04<00:00,  8.52it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 34/37 [00:04<00:00, 10.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 37/37 [00:05<00:00,  7.34it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 1/37 [00:00<00:28,  1.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "  8%|▊         | 3/37 [00:00<00:08,  3.92it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█         | 4/37 [00:01<00:10,  3.22it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▎        | 5/37 [00:01<00:08,  3.67it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 6/37 [00:01<00:09,  3.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 7/37 [00:02<00:07,  3.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 8/37 [00:02<00:08,  3.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 9/37 [00:02<00:07,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 10/37 [00:02<00:07,  3.81it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 11/37 [00:03<00:08,  3.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 12/37 [00:03<00:07,  3.14it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 14/37 [00:04<00:06,  3.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 16/37 [00:05<00:07,  2.89it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 17/37 [00:05<00:06,  3.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████▏    | 19/37 [00:05<00:04,  3.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 54%|█████▍    | 20/37 [00:06<00:05,  3.06it/s]\u001b[A\u001b[A\n",
            "\n",
            " 59%|█████▉    | 22/37 [00:06<00:03,  3.94it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 23/37 [00:06<00:04,  3.40it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 24/37 [00:07<00:04,  3.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 26/37 [00:07<00:03,  3.61it/s]\u001b[A\u001b[A\n",
            "\n",
            " 73%|███████▎  | 27/37 [00:07<00:02,  3.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 28/37 [00:08<00:02,  3.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 29/37 [00:08<00:02,  3.87it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 30/37 [00:08<00:01,  3.63it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 32/37 [00:09<00:01,  3.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 33/37 [00:09<00:01,  3.43it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 34/37 [00:10<00:00,  3.24it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 36/37 [00:10<00:00,  4.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 37/37 [00:10<00:00,  3.36it/s]\n",
            "\n",
            "\n",
            "  0%|          | 0/37 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  3%|▎         | 1/37 [00:01<00:42,  1.18s/it]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█         | 4/37 [00:01<00:14,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 14%|█▎        | 5/37 [00:02<00:11,  2.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 6/37 [00:02<00:11,  2.76it/s]\u001b[A\u001b[A\n",
            "\n",
            " 19%|█▉        | 7/37 [00:02<00:10,  2.73it/s]\u001b[A\u001b[A\n",
            "\n",
            " 22%|██▏       | 8/37 [00:03<00:09,  2.90it/s]\u001b[A\u001b[A\n",
            "\n",
            " 24%|██▍       | 9/37 [00:03<00:08,  3.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 27%|██▋       | 10/37 [00:04<00:11,  2.45it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 11/37 [00:04<00:08,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▌      | 13/37 [00:04<00:06,  3.59it/s]\u001b[A\u001b[A\n",
            "\n",
            " 38%|███▊      | 14/37 [00:05<00:07,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████      | 15/37 [00:05<00:06,  3.51it/s]\u001b[A\u001b[A\n",
            "\n",
            " 43%|████▎     | 16/37 [00:05<00:06,  3.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 17/37 [00:05<00:05,  3.55it/s]\u001b[A\u001b[A\n",
            "\n",
            " 49%|████▊     | 18/37 [00:06<00:06,  2.95it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████▏    | 19/37 [00:06<00:05,  3.48it/s]\u001b[A\u001b[A\n",
            "\n",
            " 57%|█████▋    | 21/37 [00:07<00:05,  3.13it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 23/37 [00:07<00:03,  3.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 68%|██████▊   | 25/37 [00:08<00:04,  2.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 26/37 [00:08<00:03,  3.33it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 28/37 [00:08<00:02,  4.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 29/37 [00:09<00:02,  3.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 30/37 [00:09<00:02,  3.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 86%|████████▋ | 32/37 [00:10<00:01,  3.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 89%|████████▉ | 33/37 [00:10<00:01,  2.99it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 36/37 [00:11<00:00,  4.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 37/37 [00:11<00:00,  3.14it/s]\n"
          ]
        }
      ],
      "source": [
        "llm = OpenAI(api_key=openai_key, model=\"gpt-3.5-turbo\")\n",
        "extractors = [\n",
        "    KeywordExtractor(keywords=5, llm=llm),\n",
        "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
        "    SummaryExtractor(llm=llm),\n",
        "]\n",
        "\n",
        "pipeline = IngestionPipeline(\n",
        "    transformations=extractors,\n",
        ")\n",
        "nodes = await pipeline.arun(nodes=nodes, in_place=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfcnGHHgfwvW"
      },
      "source": [
        "### ***choose an embedding model to use for the vector store:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvPm4Mu7fwvW"
      },
      "source": [
        "#### **OpenAI Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MDzkV6OWfwvW"
      },
      "outputs": [],
      "source": [
        "model_ada=\"text-embedding-ada-002\"\n",
        "small_txt_embedmodel_=\"text-embedding-3-small\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sH2VwgLYfwvW"
      },
      "outputs": [],
      "source": [
        "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", api_key=openai_key)\n",
        "\n",
        "for node in nodes:\n",
        "    actual_text = node.get_content(metadata_mode=\"all\")\n",
        "\n",
        "    node_embedding = embed_model.get_text_embedding(actual_text)\n",
        "\n",
        "    node.embedding = node_embedding\n",
        "\n",
        "    node.metadata[\"text_content\"] = actual_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCHE4Rx3fwvW"
      },
      "source": [
        "### **load the embeddings into the vector store (e.g. create a vector store):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710,
          "referenced_widgets": [
            "e3b897091ba7456d8a78ae645b3053a5",
            "c3f0569cc52e47eb888fb07d9c005b42",
            "9049229cfb5c4637bb5a6a3fc8cab760",
            "1a2437e39fd14e51b5b5b0024a8774c0",
            "9e54021ffa324a51a724fa52e4027a8f",
            "186ca154b4f84d41a200eff00e3b4d23",
            "852d9302192e49408d7c50a99adf5c9a",
            "1a31a8d8bc534f47a8a6626784a8f247",
            "c641fb1d394346449552a263eda45177",
            "d0ea50f335c545c898afcceab9e11380",
            "a9c9cd9c74e848c6b26a47bf0b367e46"
          ]
        },
        "id": "Rm6xAC8mfwvW",
        "outputId": "84b394f9-409c-45e8-e04c-68e8fb8833ec"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upserted vectors:   0%|          | 0/37 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3b897091ba7456d8a78ae645b3053a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['d373d7dd-3467-4a1d-ab17-67f3dc090078',\n",
              " '2404cdb6-9799-4fd9-91aa-3c13c51688d8',\n",
              " 'effa29cc-1fb2-4fde-aeeb-d212ccc78ab4',\n",
              " '250eab60-9109-4afe-a2d3-b39784bccf75',\n",
              " 'bae3dca9-f82a-4bbd-9d12-bb2c4aa741cc',\n",
              " '7a4336f5-a8aa-4b9f-9e4f-918f06facca0',\n",
              " '22a2c44a-eabc-47f6-8b10-d1ad8d627183',\n",
              " '50388681-d7c6-4f12-91d4-b0b1ab635cd4',\n",
              " '88b28227-bf37-4ba8-93b3-95f2360a0d19',\n",
              " 'e6b0a8d4-a6fe-43cf-b210-d64aa4f7a2bd',\n",
              " 'f3d45136-8626-479e-9d9f-36d39c080a8c',\n",
              " '97c6f512-0217-4277-80f3-545ef5e4b1f7',\n",
              " '30e4747a-2855-4813-95b3-006803f290fd',\n",
              " '8a5f77a3-ec7b-464c-9d56-7f86d136764b',\n",
              " '2a0a2e88-ea1c-4287-9612-05da99d67bc0',\n",
              " '1b4e58e0-2001-45c5-bc92-79f44046843f',\n",
              " 'dfb059fb-b8c7-492c-b002-67ceb00f783d',\n",
              " '9c2c9f2f-b8b1-45cd-83f0-4e2fce874eed',\n",
              " 'f1f79504-0db6-4517-b85b-394edfcbf4b7',\n",
              " '53fb8332-294b-4fec-8743-a91a3ee17b79',\n",
              " '4c5d5199-db9a-4ba6-b118-ac0d6d8dd9e6',\n",
              " '71f36167-8b6f-49fe-a7c4-33c27d4ba627',\n",
              " 'b9b488bc-22a7-46c1-9bbf-709864b0f14e',\n",
              " 'f0854b05-b3cb-4f14-b7a7-e0ceff519c31',\n",
              " '1996b68b-d788-4288-9f29-a41d84617258',\n",
              " '716e7b0e-a6f5-4a3e-af77-4f6fde479898',\n",
              " 'be383104-e4bf-4edd-97af-67bb93a0d30b',\n",
              " '2dcd17f8-edb2-4f19-a237-a3e65847c622',\n",
              " '954c6734-e42f-4542-9b43-4b06d9b3ec6e',\n",
              " '2a677868-727b-4c64-802e-a6a1ce6a0497',\n",
              " 'f4861d95-8a69-40ed-a46d-e4895c9efa38',\n",
              " '4b2788b2-69fb-4504-af9c-5824cd732007',\n",
              " '38cb3c57-f59c-44de-b2ce-3ccca5d65588',\n",
              " '7a7e92ae-1d9e-40f0-b2dc-3526683bf155',\n",
              " '828e125c-4a26-414f-a775-8ba1ff9ab58f',\n",
              " '686be042-75dc-45ab-b18d-4f2f981efc9d',\n",
              " 'd328d538-2f46-4072-89ed-57f5ad75cb69']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "vector_store.add(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A5fd5-1fwvX",
        "outputId": "9dee27bc-3a57-4dcc-9695-af46d5c6e3e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "pc_index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp0XgpbNfwvX",
        "outputId": "f797d464-4ea0-4ea4-9b3c-f92812e90af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'excerpt_keywords': 'Breville, Espresso Machine, Specialty Coffee, Grinder, Latte Art', 'questions_this_excerpt_can_answer': '1. How does the Breville BES870XL Espresso Machine contribute to the third wave specialty coffee experience at home?\\n2. What specific features of the Breville Barista Express make it stand out as an all-in-one espresso machine with an integrated grinder?\\n3. What accessories are included with the Breville BES870XL Espresso Machine, and how do they enhance the overall coffee-making experience for users?', 'section_summary': 'The key topics of the section include the Breville BES870XL Espresso Machine, its features such as dose control grinding, optimal water pressure, precise espresso extraction, and manual microfoam milk texturing for latte art. The section also mentions the included accessories, capacity, settings, and warranty of the machine. Additionally, it highlights how the Breville Barista Express contributes to the third wave specialty coffee experience at home with its integrated grinder and user-friendly design. Key entities mentioned are Breville, Espresso Machine, Specialty Coffee, Grinder, and Latte Art.', 'text_content': \"[Excerpt from document]\\nexcerpt_keywords: Breville, Espresso Machine, Specialty Coffee, Grinder, Latte Art\\nquestions_this_excerpt_can_answer: 1. How does the Breville BES870XL Espresso Machine contribute to the third wave specialty coffee experience at home?\\n2. What specific features of the Breville Barista Express make it stand out as an all-in-one espresso machine with an integrated grinder?\\n3. What accessories are included with the Breville BES870XL Espresso Machine, and how do they enhance the overall coffee-making experience for users?\\nsection_summary: The key topics of the section include the Breville BES870XL Espresso Machine, its features such as dose control grinding, optimal water pressure, precise espresso extraction, and manual microfoam milk texturing for latte art. The section also mentions the included accessories, capacity, settings, and warranty of the machine. Additionally, it highlights how the Breville Barista Express contributes to the third wave specialty coffee experience at home with its integrated grinder and user-friendly design. Key entities mentioned are Breville, Espresso Machine, Specialty Coffee, Grinder, and Latte Art.\\nExcerpt:\\n-----\\n94-844: Generative AI Labs \\n \\nProduct Information \\n \\n1. Category: Home and Kitchen \\n \\nWe chose this category because it is the most popular, as well as the highest \\ncontributing category on Amazon. \\n \\nProduct: Breville BES870XL Espresso Machine, One Size, Brushed Stainless Steel \\n \\nDescription: \\nAbout this item \\nThe Breville Barista Express delivers third wave specialty coGee at home using the 4 \\nkeys formula and is part of the Barista Series that oGers all in one espresso \\nmachines with integrated grinder to go from beans to espresso in under one minute \\nDOSE CONTROL GRINDING: Integrated precision conical burr grinder grinds on \\ndemand to deliver the right amount of freshly ground coGee directly into the \\nportaﬁlter for your preferred taste with any roast of bean \\nOPTIMAL WATER PRESSURE: Low pressure pre-infusion gradually increases \\npressure at the start and helps ensure all the ﬂavors are drawn out evenly during the \\nextraction for a balanced tasting cup \\nPRECISE ESPRESSO EXTRACTION: Digital temperature control (PID) delivers water \\nat precisely the right temperature, ensuring optimal espresso extraction \\nMANUAL MICROFOAM MILK TEXTURING: The powerful steam wand performance \\nallows you to hand texture microfoam milk that enhances ﬂavor and enables \\ncreation of latte art \\nESPRESSO MACHINE WITH GRIND SIZE DIAL: Simple and intuitive, giving you \\ncontrol over the grind size no matter what type of bean you're grinding \\nESPRESSO MAKER WITH BUILT-IN COFFEE GRINDER: Innovative grinding cradle \\nallows any at home barista to grind directly into the espresso portaﬁlter for the \\nperfect espresso \\nINCLUDED ACCESSORIES: Razor Dose Trimming Tool, 54mm Stainless Steel \\nPortaﬁlter, 1 & 2 cup Single & Dual Wall Filter Baskets, CoGee Scoop, Integrated \\nTamper, Stainless Steel Milk Jug, Cleaning Disc, Tablets, Brush Tool & Allen Key, \\nWater Filter & Filter Holder \\nCAPACITY & SETTINGS: 1/2 lb Bean Hopper; 67 oz Water Tank; Single or double \\nshots; Adjustable Grind Amount and Manual Override \\nWARRANTY: 1 Year Limited Product Warranty; Power: 1600 Watts; Voltage: 120 Volts \\n \\nReviews: \\n \\n5.0 out of 5 stars great product, one of the best purchases if you drink coGee \\nReviewed in the United States on May 22, 2016\\n-----\"}\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UzQyTm5fwvX",
        "outputId": "95381bf5-5400-4898-df38-718e760ee0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node ID: d373d7dd-3467-4a1d-ab17-67f3dc090078\n",
            "Text: 94-844: Generative AI Labs    Product Information    1.\n",
            "Category: Home and Kitchen    We chose this category because it is the\n",
            "most popular, as well as the highest  contributing category on Amazon.\n",
            "Product: Breville BES870XL Espresso Machine, One Size, Brushed\n",
            "Stainless Steel    Description:  About this item  The Breville Barista\n",
            "Express deli...\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLNtfc5_fwvX"
      },
      "source": [
        "### **Retrieve Content from the Vector Store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gz0Q4qosfwvX"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJo96lI4fwvX"
      },
      "source": [
        "## General Prompt & Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwWohAdHfwvX"
      },
      "source": [
        "#### Coffee Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f6kCQ9bfwvX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2QkvouyfwvY"
      },
      "source": [
        "#### Meta Quest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOJzs2hhfwvY",
        "outputId": "9f02cc32-7050-4116-8273-5c6ba24664b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "The Meta Quest 3 is a virtual and augmented reality headset that offers an elevated gaming experience. It sports an enhanced room scanning tool that uses multiple cameras to build virtual spaces. This enables users to not only avoid physical obstacles in their play area but also interact with digital counterparts of real-world objects. Additionally, it allows streaming games from a gaming PC to the headset, which can be a boon for users with vision issues since it allows the screen to be the size of a movie theater.\n",
            "\n",
            "The device also boasts improved resolution, lessening the \"screen door\" effect seen in previous iterations like the Quest 1. On the downside, it is reported to use up its battery quickly, especially when using Augmented Reality features, so keep a charging cable close by.\n",
            "\n",
            "The Quest 3 comes with a built-in strap which some users find uncomfortable, recommending a third-party strap instead for a better fit. It should be noted that the Quest Link software, which connects the VR headset to the PC, has been described by users as unreliable and tricky to troubleshoot, despite the device's high performance when used wirelessly.\n",
            "\n",
            "Regarding the visuals, Quest 3 has shown a significant improvement in standalone gaming due to its usage of \"pancake lens,\" which contributes to better graphics. The device also offers mixed reality gaming, offering a variety of mini-games for an immersive experience.\n",
            "\n",
            "Please note that most of these are derived from user reviews and might be subjective in nature. Always consider checking official technical specifications for any device's capabilities and potential issues.\n"
          ]
        }
      ],
      "source": [
        "query1 = \"Find information specifically about Meta Quest only.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided information to answer the user's question.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"The retrieved information is:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Provide me a more wholistic understanding (description of its look, feature, and possible failure) of Meta Quest 3\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aA1IcdefwvY"
      },
      "source": [
        "#### Handpan Drum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3svIF2QfwvY",
        "outputId": "dd3f40dc-bb8a-41a0-8c2d-da5fd9600c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "The Handpan Drum is a unique musical instrument appreciated for its outstanding sound quality and beautiful tones. A standard Handpan Drum includes a hollow, pan-like body constructed from durable materials such as steel, and multiple tone fields each tuned to specific notes. \n",
            "\n",
            "In terms of looks, the drum has a distinct UFO-like shape. The top (\"Ding\") side has a center note hammered into it and usually around seven to nine 'tone fields'. The bottom (\"Gu\") side often contains a round opening, producing a resonant bass sound when struck.\n",
            "\n",
            "The key features of the instrument include its easy-to-play nature, its calming and ethereal sound, and the note arrangement which allows one to create beautiful melodies simply by tapping rhythmically with the hands. Different manufacturers may include accessories such as carrying cases, stands, and drum sticks.\n",
            "\n",
            "However, possible failures could involve issues with tuning or sound quality over time. Some user reviews mentioned that some notes might start to fall out of tune after some\n"
          ]
        }
      ],
      "source": [
        "query1 = \"Find information specifically about Handpan Drum only.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=10,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided information to answer the user's question.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"The retrieved information is:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Provide me a more wholistic understanding (description of its look, feature, and possible failure) of Handpan Drum\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqJitYLDfwvY"
      },
      "source": [
        "## Visual Information & Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jVaWSVfwvY"
      },
      "source": [
        "#### Coffee Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GmZgADzfwvZ",
        "outputId": "dc2776b0-ef73-4f28-f8c2-d785b2215f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "The Breville espresso machine features several visual qualities:\n",
            "\n",
            "- Design: The machine is carefully crafted with thoughtful details and high-quality materials. Its sleek and sturdy design is appealing. Notable features include a warm storage surface for the portafilter and coffee cup, a hidden accessories tray, and a magnetized hole for the tamper.\n",
            "\n",
            "- Performance: Breville espresso machine products are built for consistency. They are designed to grind, dose, and extract all in one, enabling you to go from beans to espresso within a minute. The controls of the machine are easy to use. \n",
            "\n",
            "- Accessories: The machine comes along with a variety of accessories such as a razor dose trimming tool, portafilter, filter baskets, coffee scoop, integrated tamper, stainless steel milk jug, cleaning disc, tablets, brush tool and Allen key, water filter and filter holder. \n",
            "\n",
            "- Built-In Grinder: The Breville Barista Express includes an integrated precision conical burr grinder, which grinds on demand, delivering a specific amount of ground coffee for each use. \n",
            "\n",
            "- Size adjustment: The machine also gives the user the freedom to adjust the size of the coffee grind to suit personal preference.\n",
            "\n",
            "- Customer Satisfaction: The machine's features and build quality have earned it numerous positive reviews from users. A particular user has expressed that Breville provides excellent customer service and support even when issues arise after the warranty period showcasing their commitment to product quality and customer satisfaction.\n"
          ]
        }
      ],
      "source": [
        "query1 = \"Find information specifically about Breville espresso machiness only.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided information to answer the user's question.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"The retrieved information is:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Provide me some bullet points of visual informations of Breville espresso machiness \\\n",
        "                  that helps generating product image\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Find information specifically about Breville espresso machiness only.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Use the provided information to answer the user's question.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"The retrieved information is:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"What is the answer?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Give me a wholistic understanding of Breville espresso machines\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "id": "8UynS0w9uj7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specializing in providing insights and summaries based on the provided information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Provide a 5 line short summary of Breville espresso machines, highlighting key features, design, \\\n",
        "                  and user benefits which are only useful for image generation\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxdYJt8KzLBp",
        "outputId": "a9d128f7-11ff-47a7-f38f-70cde9de9731"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Breville Espresso Machines are meticulously designed, high-grade brewing appliances that are praised for delivering quality espresso consistently. One of the key features that stands out is their in-built grinding mechanism, which allows freshly ground coffee beans to be used, thereby enhancing the flavor and richness of the brewed espresso. \n",
            "\n",
            "The machines are compact, making them an easy fit in various kitchen setups without occupying excessive space. Their exteriors and interior displays are convenient, stylized, and pleasing to the eye, indicating that Breville values both form and function. Notably, Breville espresso machines are equipped with precise temperature control (PID) for optimal espresso extraction and a steam wand for manual microfoam milk texturing for latte art. \n",
            "\n",
            "In terms of user benefits, these espresso machines are lauded for their user-friendly controls and ease of use. Breville offers enough customization options, including grind size adjustment and extraction preferences, that consistently ensure the perfect shot of espresso. To assist novice users, the machines come with a well-curated user instructions manual.\n",
            "\n",
            "Despite being advanced machines, they are also commended for their durability even with regular use. For customer satisfaction, Breville provides excellent customer service even beyond warranty periods, consolidating user trust in the product reliability. The machines also gain points for their affordability relative to their high functionality and the quality of coffee they produce. \n",
            "\n",
            "In summary, Breville's espresso machines exemplify well-crafted design, reliable durability, and user-centric functionality, making them a worthy investment for coffee lovers. This description summarizes the design, function, and user benefits of Breville espresso machines for potential image generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specializing in providing insights and summaries based on the provided information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Summarize Breville espresso machines based on user reviews, highlighting visual aspects \\\n",
        "                  like design, material finish, steam wand, digital display, and how it fits into kitchen \\\n",
        "                  spaces.\"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7U9tkpg3r0k",
        "outputId": "37400aa7-05c3-4dc4-f3b2-9d031eaaedc5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Based on numerous user reviews, majority of users commend the design, functionality, and durability of Breville espresso machines. Many users assert that Breville machines have a sleek, modern aesthetic with a robust stainless steel finish, giving them a high-end look that fits well in a variety of kitchen spaces. They also appreciate the compact size of the machines, which doesn't dominate countertop space, yet maintains a distinctive presence.\n",
            "\n",
            "The Breville model discussed, known as the Barista Express, houses a variety of features such as the steam wand, which many users have praised for its richness and creaminess in producing milk foam. They consider this factor highly important for making lattes and cappuccinos. Another lauded aspect is the digital temperature control (PID), which ensures more precise water temperature, contributing to optimal espresso extraction.\n",
            "\n",
            "These espresso machines also come with an integrated grinder - a feature that users highly appreciate. This detail allows you to grind on demand and deliver the right amount of freshly ground coffee directly into the porta-filter. This capability is further enhanced by the grind size dial, a feature that provides further control over the grind size, no matter the type of bean used.\n",
            "\n",
            "In terms of functionality and fit within kitchen spaces, users found the Breville machines to be extremely well designed with user-friendly controls. They also praised the hidden accessories tray and the top surface of the machine, which stays warm from the boiler to keep the portafilter and coffee cup warm. Even the instructions were praised for their comprehensibility and usefulness.\n",
            "\n",
            "On the topic of durability, one user mentioned that after using the machine 4-5 times a week for a year, it still held up remarkably well. They stated that there were no issues or malfunctions, showing their confidence in the quality and longevity of Breville machines.\n",
            "\n",
            "However, something that a few users have stressed is the importance of reading the user instructions before using the device to avoid future issues. They also suggest using fresh espresso whole beans that are 100% Arabic and not oily to prevent the grinder from gumming up.\n",
            "\n",
            "In conclusion, considering user reviews, Breville espresso machines are generally well-received for their appealing design, reliable performance, user-friendly controls, and thoughtful construction. They seem to provide an exceptional balance of form, functionality, and durability, making them a great fit for a variety of kitchen designs and user preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specializing in providing insights and summaries based on the provided information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Give description of Breville espresso machines highlighting physical appearance \\\n",
        "                   and only visual aspects of user reviews in less than 20 words \"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpzK7Av34qNW",
        "outputId": "40fac765-0032-40f4-80e9-5c6fe914e71f"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Breville espresso machines showcase a sleek, stainless steel design, seamlessly blending in any kitchen setting. User reviews often mention their high-quality construction, ease of use, and consistent espresso production.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specializing in providing insights and summaries based on the provided information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Give description of Breville espresso machines highlighting physical appearance \\\n",
        "                   and only visual aspects of user reviews in less than 30 words \"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJc84boY78xI",
        "outputId": "7b9aa791-0308-4976-ae4e-bf4744a2ae05"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Breville espresso machines are compact, sleek in stainless steel design, and fit comfortably in any kitchen. They feature an integrated grinder and intuitive controls. User reviews praise its consistent performance, ease of use, durability, and customization options. Their attractive design and high-quality construction are frequently mentioned as highlights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert assistant specializing in providing insights and summaries based on the provided information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Give description of Breville espresso machines highlighting physical appearance \\\n",
        "                   and only visual aspects of user reviews(positive and negative) in less than 30 words \"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0xWG1wV73Rv",
        "outputId": "28def38f-7a34-42c8-92f2-8a7c2fa6af78"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Breville espresso machines stand out with their sleek, stainless steel design, offering strength and ease of use. They are compact, capable of fitting into any kitchen setup, and feature various intuitive controls. User reviews note the consistency of the espresso produced, the quick heating, and easy-to-adjust grind levels. However, some users highlight that getting the grind level just right can be challenging and that the machine may run out of water during operation. Moreover, potential premature failure of the grinder is a concern.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"Retrieve detailed and relevant information about the design, functionality, and user experience of Breville espresso machines.\"\n",
        "\n",
        "query_response = client.embeddings.create(\n",
        "    model=\"text-embedding-ada-002\",\n",
        "    input=[query1]\n",
        ")\n",
        "\n",
        "query_embedding = query_response.data[0].embedding\n",
        "\n",
        "res2 = pc_index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=5,\n",
        "    include_metadata=True\n",
        ")\n",
        "\n",
        "retrieved_texts = []\n",
        "for match in res2['matches']:\n",
        "    actual_text = match['metadata'].get('text_content', 'No text available')\n",
        "    retrieved_texts.append(actual_text)\n",
        "\n",
        "combined_context = \"\\n\\n\".join(retrieved_texts)\n",
        "\n",
        "def rag_openai_gpt(model, query, retrieved_context):\n",
        "    \"\"\"\n",
        "    Reason over the retrieved context using OpenAI's GPT model.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are assisting a 3D artist to generate an image. Give more details on how to get images\"},\n",
        "        {\"role\": \"user\", \"content\": f\"The question is: {query}\"},\n",
        "        {\"role\": \"assistant\", \"content\": f\"Here is the relevant information you can use to address the query:\\n\\n{retrieved_context}\"},\n",
        "        {\"role\": \"user\", \"content\": \"Can you provide a detailed, comprehensive response to the question?\"}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI API call: {e}\")\n",
        "        return \"Error: Unable to generate an answer.\"\n",
        "\n",
        "query_to_answer = \"Give description of Breville espresso machines highlighting physical appearance \\\n",
        "                   and only visual aspects of user reviews(positive and negative) in less than 30 words \"\n",
        "model_to_use = \"gpt-4\"\n",
        "final_answer = rag_openai_gpt(model_to_use, query_to_answer, combined_context)\n",
        "\n",
        "print(\"Final Answer:\")\n",
        "print(final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4KCE4Xq8pcj",
        "outputId": "f642a91a-39e0-4728-d5be-2a51e78b20dc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Answer:\n",
            "Breville espresso machines are robust and elegantly designed products known for their high-quality stainless steel built. Users praise them for consistent espresso production, user-friendly controls, and thoughtful design elements. Some users note high durability, easy usage, and impressive performance. A customized coarseness of grinds, manual micro-foam milk texturing, and easy dosage control are highly acclaimed features. However, few users indicate problems with grinder maintenance and concerns about premature failure. In summary, users find Breville espresso machines reliable and worthwhile for the exquisite coffee-making experience they provide.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZShXmej9tbK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3b897091ba7456d8a78ae645b3053a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c3f0569cc52e47eb888fb07d9c005b42",
              "IPY_MODEL_9049229cfb5c4637bb5a6a3fc8cab760",
              "IPY_MODEL_1a2437e39fd14e51b5b5b0024a8774c0"
            ],
            "layout": "IPY_MODEL_9e54021ffa324a51a724fa52e4027a8f"
          }
        },
        "c3f0569cc52e47eb888fb07d9c005b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186ca154b4f84d41a200eff00e3b4d23",
            "placeholder": "​",
            "style": "IPY_MODEL_852d9302192e49408d7c50a99adf5c9a",
            "value": "Upserted vectors: 100%"
          }
        },
        "9049229cfb5c4637bb5a6a3fc8cab760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a31a8d8bc534f47a8a6626784a8f247",
            "max": 37,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c641fb1d394346449552a263eda45177",
            "value": 37
          }
        },
        "1a2437e39fd14e51b5b5b0024a8774c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ea50f335c545c898afcceab9e11380",
            "placeholder": "​",
            "style": "IPY_MODEL_a9c9cd9c74e848c6b26a47bf0b367e46",
            "value": " 37/37 [55:54&lt;00:00, 27.03it/s]"
          }
        },
        "9e54021ffa324a51a724fa52e4027a8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "186ca154b4f84d41a200eff00e3b4d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852d9302192e49408d7c50a99adf5c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a31a8d8bc534f47a8a6626784a8f247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c641fb1d394346449552a263eda45177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0ea50f335c545c898afcceab9e11380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9c9cd9c74e848c6b26a47bf0b367e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}